{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Loading input data ''' \n",
    "def seq2ngrams(seqs, n = 3):\n",
    "    return np.array([[seq[i : i + n] for i in range(len(seq))] for seq in seqs])\n",
    "\n",
    "train_df = pd.read_csv('../../train_input.csv')\n",
    "test_df = pd.read_csv('../../test_input.csv')\n",
    "\n",
    "# Find maximum length sequence\n",
    "num_train = len(train_df)\n",
    "num_test = len(test_df)\n",
    "maxlen_seq = max(train_df['length'].values.max(), test_df['length'].values.max())\n",
    "\n",
    "# Loading and converting the inputs to ngrams\n",
    "train_input_aa, train_input_q8 = train_df[['sequence', 'q8']].values.T\n",
    "train_aa_grams = seq2ngrams(train_input_aa, n=3)\n",
    "train_q8_grams = seq2ngrams(train_input_q8, n=3)\n",
    "\n",
    "test_input_aa, test_input_q8 = test_df[['sequence', 'q8']].values.T\n",
    "test_aa_grams = seq2ngrams(test_input_aa, n=3)\n",
    "test_q8_grams = seq2ngrams(test_input_q8, n=3)\n",
    "\n",
    "# Initializing and defining the tokenizer encoders based on the train set\n",
    "tokenizer_encoder_aa = Tokenizer()\n",
    "tokenizer_encoder_aa.fit_on_texts(train_aa_grams)\n",
    "tokenizer_encoder_q8 = Tokenizer()\n",
    "tokenizer_encoder_q8.fit_on_texts(train_q8_grams)\n",
    "\n",
    "# Using the tokenizer to encode the input sequences for use in training and testing\n",
    "train_input_aa = tokenizer_encoder_aa.texts_to_sequences(train_aa_grams)\n",
    "train_input_aa = sequence.pad_sequences(train_input_aa, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
    "train_input_q8 = tokenizer_encoder_q8.texts_to_sequences(train_q8_grams)\n",
    "train_input_q8 = sequence.pad_sequences(train_input_q8, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
    "\n",
    "test_input_aa = tokenizer_encoder_aa.texts_to_sequences(test_aa_grams)\n",
    "test_input_aa = sequence.pad_sequences(test_input_aa, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
    "test_input_q8 = tokenizer_encoder_q8.texts_to_sequences(test_q8_grams)\n",
    "test_input_q8 = sequence.pad_sequences(test_input_q8, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
    "\n",
    "n_words_aa = len(tokenizer_encoder_aa.word_index) + 1\n",
    "n_words_q8 = len(tokenizer_encoder_q8.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Loading training target data '''\n",
    "train_target_arrs = np.load('../../train_output.npz')\n",
    "\n",
    "seq_len = train_df['length'].values\n",
    "train_target_data = np.zeros((num_train, maxlen_seq, maxlen_seq))\n",
    "for i in range(num_train):\n",
    "    train_target_data[i, :seq_len[i], :seq_len[i]] = train_target_arrs['arr_' + str(i)]\n",
    "\n",
    "train_target_data_flat = np.zeros((num_train, int(maxlen_seq*(maxlen_seq-1)/2))) #n*(n-1)/2\n",
    "for i in range(num_train):\n",
    "    len_seq = seq_len[i]\n",
    "    tri_idx = np.triu_indices(len_seq, 1)\n",
    "    train_target_data_flat[i, :int(len_seq*(len_seq-1)/2)] = train_target_data[i, :len_seq, :len_seq][tri_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom distance layer for making predictions\n",
    "class Distance(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Distance, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Distance, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        r = tf.reduce_sum(x*x, 2)\n",
    "        r = tf.expand_dims(r, -1)\n",
    "        xx = tf.einsum('bij,bjk->bik', x, tf.linalg.transpose(x))\n",
    "        D = r - 2*xx + tf.linalg.transpose(r)\n",
    "        return D\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[1]) \n",
    "    \n",
    "class Symmetrize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Symmetrize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Symmetrize, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = (x+tf.linalg.transpose(x))/2\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[1]) \n",
    "\n",
    "class Pairwise(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Pairwise, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Pairwise, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        idx1, idx2 = np.triu_indices(maxlen_seq)\n",
    "        idx = (maxlen_seq*idx1+idx2)[np.where(idx1-idx2!=0)]\n",
    "        \n",
    "        x1 = K.expand_dims(x, 1)\n",
    "        x2 = K.expand_dims(x, 2)\n",
    "        x1 = Lambda(lambda x: K.repeat_elements(x, maxlen_seq, axis=1))(x1)\n",
    "        x2 = Lambda(lambda x: K.repeat_elements(x, maxlen_seq, axis=2))(x2)\n",
    "        x_ = Concatenate()([x1, x2])\n",
    "        x_ = Reshape([maxlen_seq*maxlen_seq, 2304])(x_)\n",
    "        x_ = tf.gather(x_, indices=idx, axis=1)\n",
    "        return x_\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(input_shape[1]-1)/2, 2*input_shape[2])\n",
    "    \n",
    "\n",
    "def model4(emb):\n",
    "    def conv(x, filter_size):\n",
    "        x = ZeroPadding2D((filter_size//2, 0), data_format='channels_first')(x)\n",
    "        x = Conv2D(filters=64, kernel_size=(filter_size, 128), data_format='channels_first', activation='relu')(x)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        return x\n",
    "\n",
    "    emb = Reshape([1, maxlen_seq, 128])(emb)\n",
    "\n",
    "    # Defining 3 convolutional layers with different kernel sizes\n",
    "    conv1 = conv(emb, 3)\n",
    "    conv2 = conv(emb, 7)\n",
    "    conv3 = conv(emb, 11)\n",
    "    conv_ = Concatenate(-1)([conv1, conv2, conv3])\n",
    "    conv_ = Permute((2, 1, 3))(conv_)\n",
    "    conv_ = Reshape([maxlen_seq, 3*64])(conv_)\n",
    "\n",
    "    # Defining 3 bidirectional GRU layers; taking the concatenation of outputs \n",
    "    gru1 = Bidirectional(GRU(64, return_sequences='True', recurrent_dropout=0.1))(conv_)\n",
    "    gru2 = Bidirectional(GRU(64, return_sequences='True', recurrent_dropout=0.1))(gru1)\n",
    "    gru3 = Bidirectional(GRU(64, return_sequences='True', recurrent_dropout=0.1))(gru2)\n",
    "    comb = Concatenate(-1)([gru1, gru2, gru3, conv_])    \n",
    "    return comb\n",
    "\n",
    "def get_model(num):\n",
    "    # Embedding inputs\n",
    "    input_aa = Input(shape = (maxlen_seq, ))\n",
    "    input_q8 = Input(shape = (maxlen_seq, ))\n",
    "\n",
    "    embed_aa = Embedding(input_dim = n_words_aa, output_dim = 128, input_length = maxlen_seq)(input_aa)\n",
    "    embed_q8 = Embedding(input_dim = n_words_q8, output_dim = 128, input_length = maxlen_seq)(input_q8)\n",
    "    \n",
    "    if num == 2:\n",
    "        aa = model2(embed_aa)\n",
    "        q8 = model2(embed_q8)\n",
    "    elif num == 4:\n",
    "        aa = model4(embed_aa)\n",
    "        q8 = model4(embed_q8)\n",
    "    \n",
    "    # concatenate features and then impose pairwise constraints\n",
    "    x = Concatenate(-1)([aa, q8]) \n",
    "    x = Pairwise()(x)\n",
    "\n",
    "    # Some final fully connected layers before distance calculation\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    y = Dense(1, use_bias=False)(x)\n",
    "    \n",
    "    # Defining the model as a whole and printing the summary\n",
    "    return Model([input_aa, input_q8], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 238395, 2304)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_69 (InputLayer)           (None, 691)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_70 (InputLayer)           (None, 691)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_69 (Embedding)        (None, 691, 128)     1225472     input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_70 (Embedding)        (None, 691, 128)     34688       input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_172 (Reshape)           (None, 1, 691, 128)  0           embedding_69[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_174 (Reshape)           (None, 1, 691, 128)  0           embedding_70[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_205 (ZeroPadding (None, 1, 693, 128)  0           reshape_172[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_206 (ZeroPadding (None, 1, 697, 128)  0           reshape_172[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_207 (ZeroPadding (None, 1, 701, 128)  0           reshape_172[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_208 (ZeroPadding (None, 1, 693, 128)  0           reshape_174[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_209 (ZeroPadding (None, 1, 697, 128)  0           reshape_174[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_210 (ZeroPadding (None, 1, 701, 128)  0           reshape_174[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 64, 691, 1)   24640       zero_padding2d_205[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 64, 691, 1)   57408       zero_padding2d_206[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 64, 691, 1)   90176       zero_padding2d_207[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 64, 691, 1)   24640       zero_padding2d_208[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 64, 691, 1)   57408       zero_padding2d_209[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 64, 691, 1)   90176       zero_padding2d_210[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 64, 691, 1)   4           conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 64, 691, 1)   4           conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 64, 691, 1)   4           conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 64, 691, 1)   4           conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 64, 691, 1)   4           conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 64, 691, 1)   4           conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_198 (Concatenate)   (None, 64, 691, 3)   0           batch_normalization_205[0][0]    \n",
      "                                                                 batch_normalization_206[0][0]    \n",
      "                                                                 batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_200 (Concatenate)   (None, 64, 691, 3)   0           batch_normalization_208[0][0]    \n",
      "                                                                 batch_normalization_209[0][0]    \n",
      "                                                                 batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "permute_82 (Permute)            (None, 691, 64, 3)   0           concatenate_198[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "permute_83 (Permute)            (None, 691, 64, 3)   0           concatenate_200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_173 (Reshape)           (None, 691, 192)     0           permute_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_175 (Reshape)           (None, 691, 192)     0           permute_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_205 (Bidirectiona (None, 691, 128)     98688       reshape_173[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_208 (Bidirectiona (None, 691, 128)     98688       reshape_175[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_206 (Bidirectiona (None, 691, 128)     74112       bidirectional_205[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_209 (Bidirectiona (None, 691, 128)     74112       bidirectional_208[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_207 (Bidirectiona (None, 691, 128)     74112       bidirectional_206[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_210 (Bidirectiona (None, 691, 128)     74112       bidirectional_209[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_199 (Concatenate)   (None, 691, 576)     0           bidirectional_205[0][0]          \n",
      "                                                                 bidirectional_206[0][0]          \n",
      "                                                                 bidirectional_207[0][0]          \n",
      "                                                                 reshape_173[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_201 (Concatenate)   (None, 691, 576)     0           bidirectional_208[0][0]          \n",
      "                                                                 bidirectional_209[0][0]          \n",
      "                                                                 bidirectional_210[0][0]          \n",
      "                                                                 reshape_175[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_202 (Concatenate)   (None, 691, 1152)    0           concatenate_199[0][0]            \n",
      "                                                                 concatenate_201[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pairwise_33 (Pairwise)          (None, 238395.0, 230 0           concatenate_202[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 238395.0, 102 2360320     pairwise_33[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 238395.0, 102 0           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 238395.0, 512 524800      dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 238395.0, 512 0           dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 238395.0, 256 131328      dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 238395.0, 256 0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 238395.0, 1)  256         dropout_50[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,115,160\n",
      "Trainable params: 5,115,148\n",
      "Non-trainable params: 12\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'nadam', loss = 'mean_squared_error')\n",
    "\n",
    "train_aa = train_input_aa[:10]\n",
    "val_aa = train_input_aa[10:20]\n",
    "\n",
    "train_q8 = train_input_q8[:10]\n",
    "val_q8 = train_input_q8[10:20]\n",
    "\n",
    "train_tar = train_target_data[:10]\n",
    "val_tar = train_target_data[10:20]\n",
    "\n",
    "model.fit([train_aa, train_q8], train_tar, \n",
    "          batch_size = 10, epochs = 50, \n",
    "          validation_data = ([val_aa, val_q8], val_tar), \n",
    "          callbacks = [EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)],\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' save model '''\n",
    "model.save('seq2mat_4.h5')\n",
    "model.save_weights('seq2mat_4_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Testing '''\n",
    "model = load_model('models/seq2mat_8.h5', custom_objects={'Symmetrize':Symmetrize})\n",
    "pred = model.predict([test_input_aa, test_input_q8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_input_aa, test_input_q8])\n",
    "out = []\n",
    "for i in range(len(pred)):\n",
    "    l = test_df['length'][i]\n",
    "    pred_ = pred[i, :l, :l]\n",
    "    out.append(pred_)\n",
    "np.savez('predictions/test8.npz', *out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIG CHANGES:\n",
    "    Predict n(n-1)/2 not n^2\n",
    "    PyBLAST: Homology. Find training example sequences closest to the sequences in the test example.\n",
    "        Use secondary structure to place these patches\n",
    "    GAN to fill in missing data (Namarta Anand)  \n",
    "    \n",
    "    Try good predictor of the average (autoML in SciKitLearn)\n",
    "        Then extend to row or column\n",
    "        Then extend to matrix\n",
    "        \n",
    "        Could offset our results by those mean\n",
    "        \n",
    "        Switch to plotting fixed values not normalized (use plasma)\n",
    "        \n",
    "    CANNOT find a function without checking the range of a function and first momentsstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Predictions '''\n",
    "pred = model.predict([test_input_aa, test_input_q8])\n",
    "# saving\n",
    "out = []\n",
    "for i in range(len(pred)):\n",
    "    l = test_df['length'][i]\n",
    "    pred_ = pred[i, :l, :l]\n",
    "    out.append(pred_)\n",
    "np.savez('test8.npz', *out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.03312780e+09, 1.23339476e+08, 1.63010100e+07, 1.35788800e+06,\n",
       "        2.08382000e+05, 6.13360000e+04, 2.81360000e+04, 1.61740000e+04,\n",
       "        7.20400000e+03, 1.06400000e+03]),\n",
       " array([  0.        ,  23.84976196,  47.69952393,  71.54928589,\n",
       "         95.39904785, 119.24880981, 143.09857178, 166.94833374,\n",
       "        190.7980957 , 214.64785767, 238.49761963]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_target_data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
